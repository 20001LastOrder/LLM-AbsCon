{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from loguru import logger\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from parser import ClevrParser\n",
    "from program_executor import (\n",
    "    programs_from_networkx,\n",
    "    networkx_from_programs,\n",
    "    set_scene,\n",
    "    evaluate,\n",
    ")\n",
    "import numpy as np\n",
    "from remote_encoder import RemoteEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abscon.abstraction import ClevrAbstractor\n",
    "from abscon.concretization import ClevrConcretizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from parser import ClevrParser\n",
    "from tqdm import tqdm\n",
    "from evaluation_utils import ClevrEvaluator, evaluate_graph_with_scene, evaluate_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"results\"\n",
    "ground_truth_folder = \"data\"\n",
    "approaches = [\"greedy\", \"mv\", \"esc\", \"escf\", \"abscon\",]\n",
    "llms = [\"gpt-4o-mini\", \"gpt-4o\", \"Meta-Llama-3.1-8B-Instruct\", \"Meta-Llama-3.1-70B-Instruct\"]\n",
    "\n",
    "dataset = \"clevr\"\n",
    "num_generations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for approach in approaches:\n",
    "    ground_truth_path = f\"{ground_truth_folder}/{dataset}.csv\"\n",
    "    \n",
    "    result = {}\n",
    "    result[\"approach\"] = approach\n",
    "    for llm in llms:\n",
    "        folder_path = f\"{folder}/{llm}\"\n",
    "        evaluator = ClevrEvaluator(folder_path=folder_path, dataset_name=dataset)\n",
    "\n",
    "        if approach == \"greedy\":\n",
    "            metrics = evaluator.evaluate_greedy_result()\n",
    "        elif approach == \"esc\": \n",
    "            metrics = evaluator.evaluate_execution_sc(num_generations)\n",
    "        elif approach == \"escf\":\n",
    "            metrics = evaluator.evaluate_execution_sc(num_generations, exclude_error=True)\n",
    "        else:\n",
    "            df = pd.read_csv(f\"{folder_path}/{dataset}/results_{approach}_{num_generations}.csv\")[\"0\"].tolist()\n",
    "            metrics = evaluator.evaluate_solutions(df)\n",
    "        result[f\"ACC_{llm}\"] = metrics[\"accuracy\"]\n",
    "        result[f\"SR_{llm}\"] = metrics[\"success_rate\"]\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df[results_df.select_dtypes(include=['number']).columns] *= 100\n",
    "# results_df = results_df[[\"approach\", \"success_rate\", \"accuracy\"]]\n",
    "# results_df.columns = [\"approach\", \"SR\", \"ACC\"]\n",
    "print(results_df.round(2).to_latex(index=False, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df\n",
    "results_df_index = results_df.set_index(\"approach\", inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = results_df_index.loc[\"abscon\", :] - results_df_index.loc[\"greedy\", :]\n",
    "acc_diff = [value for key, value in diff.items() if \"ACC\" in key]\n",
    "print(f\"Min improvement: {min(acc_diff)}, max improvement: {max(acc_diff)}, average improvement: {np.mean(acc_diff)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = results_df_index.loc[\"abscon\", :] - results_df_index.loc[\"escf\", :]\n",
    "acc_diff = [value for key, value in diff.items() if \"ACC\" in key]\n",
    "print(f\"Min improvement: {min(acc_diff)}, max improvement: {max(acc_diff)}, average improvement: {np.mean(acc_diff)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(dataset, llms, num_generation, approaches, folder):\n",
    "    result = {}\n",
    "    for approach in approaches:\n",
    "        result[approach] = {}\n",
    "        for llm in llms:\n",
    "            folder_path = f\"{folder}/{llm}\"\n",
    "            evaluator = ClevrEvaluator(\n",
    "                folder_path=folder_path,\n",
    "                dataset_name=dataset,\n",
    "            )\n",
    "\n",
    "            if approach == \"greedy\":\n",
    "                metrics = evaluator.evaluate_greedy_result()\n",
    "            elif approach == \"esc\":\n",
    "                metrics = evaluator.evaluate_execution_sc(num_generation)\n",
    "            elif approach == \"escf\":\n",
    "                metrics = evaluator.evaluate_execution_sc(num_generation, exclude_error=True)\n",
    "            elif approach == \"best\":\n",
    "                metrics = evaluator.evaluate_execution_sc(\n",
    "                    num_generation, exclude_error=True, best_answer=True\n",
    "                )\n",
    "            else:\n",
    "                df = pd.read_csv(\n",
    "                    f\"{folder_path}/{dataset}/results_{approach}_{num_generation}.csv\"\n",
    "                )[\"0\"].tolist()\n",
    "                metrics = evaluator.evaluate_solutions(df)\n",
    "            result[approach][llm] = metrics\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from evaluation_utils import ClevrEvaluator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"results\"\n",
    "ground_truth_folder = \"data\"\n",
    "approaches = [\"mv\", \"greedy\", \"abscon\", \"esc\", \"escf\", \"best\"]\n",
    "llms = [\"Meta-Llama-3.1-8B-Instruct\", \"Meta-Llama-3.1-70B-Instruct\"]# [\"Meta-Llama-3.1-70B-Instruct\"]\n",
    "\n",
    "dataset = \"clevr\"\n",
    "num_generations = range(1, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for num_generation in tqdm(num_generations):\n",
    "    result = get_result(dataset, llms, num_generation, approaches, folder)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "import matplotlib\n",
    "plt.style.use(['science', \"ieee\"])\n",
    "\n",
    "models = [\"Meta-Llama-3.1-8B-Instruct\"] #[\"Meta-Llama-3.1-70B-Instruct\"]\n",
    "model_names = [\"Llama3.1 70b\"]\n",
    "approaches = [\"escf\", \"abscon\", \"best\", \"esc\", \"greedy\"]\n",
    "approach_names = [\"ESC-F\", \"AbsCon\", \"Best\", \"Median\", \"Direct\"]\n",
    "lines = [\"-\", \"-\", \"--\", \"--\", \"-\"]\n",
    "markers = ['*', '.', '^', 'v', '']\n",
    "\n",
    "colors = [[33, 25, 24], [195, 56, 40], [71, 133, 90] , [71, 133, 90],  [231, 189, 57]]\n",
    "colors = [[c / 255 for c in color] for color in colors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,1.5))\n",
    "metric = \"accuracy\"\n",
    "x = num_generations\n",
    "f1_values = []\n",
    "for i, llm in enumerate(models):\n",
    "    for j, approach in enumerate(approaches):\n",
    "        values = [data[approach][llm][metric] for data in results]\n",
    "        if approach in [\"escf\", \"abscon\"]:\n",
    "            f1_values.extend(values)\n",
    "        plt.plot(x, values, color=colors[j], linestyle=lines[j], label=approach_names[j], marker=markers[j])\n",
    "plt.legend(shadow=True, ncol=2)\n",
    "plt.title(\"Clevr\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Candidates\")\n",
    "plt.savefig(\"Clevr.png\", dpi=300)\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ3: Impact of Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_results(results):\n",
    "    transformed = []\n",
    "    for temperature in results.keys():\n",
    "        temperature_result = {\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        for approach in results[temperature].keys():\n",
    "            for llm in results[temperature][approach].keys():\n",
    "                for metric in results[temperature][approach][llm].keys():\n",
    "                    temperature_result[f\"{metric}_{llm}\"] = results[temperature][\n",
    "                        approach\n",
    "                    ][llm][metric] \n",
    "        transformed.append(temperature_result)\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"results\"\n",
    "ground_truth_folder = \"data\"\n",
    "approaches = [\"abscon\"]\n",
    "llms = [\"Meta-Llama-3.1-70B-Instruct\", \"gpt-4o-mini\"]\n",
    "\n",
    "dataset = \"clevr\"\n",
    "num_generation = 10\n",
    "\n",
    "temperatures = [\"0.2\", \"0.5\", \"0.7\", \"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_results = {}\n",
    "\n",
    "for temperature in temperatures:\n",
    "    temperature_folder = f\"{folder}/temperature/{temperature}\"\n",
    "    temperature_results[temperature] = get_result(\n",
    "        dataset, llms, num_generation, approaches, temperature_folder\n",
    "    )\n",
    "temperature_results = transform_results(temperature_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(temperature_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(temperature_results)\n",
    "df[df.select_dtypes(include=[\"number\"]).columns] *= 100\n",
    "df = df[\n",
    "    [\n",
    "        \"temperature\",\n",
    "        \"accuracy_Meta-Llama-3.1-70B-Instruct\",\n",
    "        \"success_rate_Meta-Llama-3.1-70B-Instruct\",\n",
    "        \"accuracy_gpt-4o-mini\",\n",
    "        \"success_rate_gpt-4o-mini\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(df.round(2).to_latex(index=False, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abscon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
