{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from loguru import logger\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_utils import TaxonomyEvaluator\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"results\"\n",
    "ground_truth_folder = \"data\"\n",
    "approaches = [\"greedy\", \"mv\", \"abscon\"]\n",
    "llms = [\"gpt-4o-mini\", \"gpt-4o\", \"Meta-Llama-3.1-8B-Instruct\", \"Meta-Llama-3.1-70B-Instruct\"]\n",
    "\n",
    "dataset = \"ccs\"\n",
    "num_generation = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for approach in approaches:\n",
    "    ground_truth_path = f\"{ground_truth_folder}/{dataset}.csv\"\n",
    "\n",
    "    result = {}\n",
    "    result[\"approach\"] = approach\n",
    "    for llm in llms:\n",
    "        folder_path = f\"{folder}/{llm}\"\n",
    "        if approach == \"greedy\":\n",
    "            evaluator = TaxonomyEvaluator(\n",
    "                folder_path,\n",
    "                dataset,\n",
    "                ground_truth_path,\n",
    "                num_generation,\n",
    "                evaluate_greedy=True,\n",
    "            )\n",
    "            df = pd.read_csv(f\"{folder_path}/{dataset}/results_{approach}.csv\")\n",
    "            metrics = evaluator.evaluate_abstraction(\n",
    "                num_generation, concretization_method=\"mv\", dataset=dataset\n",
    "            )\n",
    "        else:\n",
    "            evaluator = TaxonomyEvaluator(\n",
    "                folder_path, dataset, ground_truth_path, num_generation\n",
    "            )\n",
    "            df = pd.read_csv(\n",
    "                f\"{folder_path}/{dataset}/results_{approach}_{num_generation}.csv\"\n",
    "            )\n",
    "            metrics = evaluator.evaluate_taxonomies(df, dataset)\n",
    "        \n",
    "\n",
    "        for metric_name in [\"precision\", \"recall\", \"f1\", \"consistency\"]:\n",
    "            result[f\"{metric_name}_{llm}\"] = metrics[metric_name]\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df[results_df.select_dtypes(include=['number']).columns] *= 100\n",
    "# results_df = results_df[[\"approach\", \"precision\", \"recall\", \"f1\", \"consistency\"]]\n",
    "# results_df.columns = [\"approach\", \"P\", \"R\", \"F1\", \"Con\"]\n",
    "print(results_df.round(2).to_latex(index=False, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_index = results_df.set_index(\"approach\", inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = results_df_index.loc[\"abscon\", :] - results_df_index.loc[\"greedy\", :]\n",
    "recall_diff = [value for key, value in diff.items() if \"recall\" in key]\n",
    "print(f\"Min improvement: {min(recall_diff)}, max improvement: {max(recall_diff)}, average improvement: {np.mean(recall_diff)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_diff = [value for key, value in diff.items() if \"f1\" in key]\n",
    "print(f\"Min improvement: {min(f1_diff)}, max improvement: {max(f1_diff)}, average improvement: {np.mean(f1_diff)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"results\"\n",
    "ground_truth_folder = \"data\"\n",
    "approaches = [\"mv\", \"greedy\", \"abscon\"]\n",
    "llms = [\"meta-llama-3-1-70b-instruct-20241203161536\"] # [\"Meta-Llama-3.1-70B-Instruct\"]\n",
    "\n",
    "dataset = \"ccs\"\n",
    "num_generations = range(1, 21)\n",
    "metrics = [\"f1\", \"consistency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(ground_truth_folder, dataset, llms, num_generation, approaches, folder):\n",
    "    ground_truth_path = f\"{ground_truth_folder}/{dataset}.csv\"\n",
    "    result = {}\n",
    "    for approach in approaches:\n",
    "        result[approach] = {}\n",
    "        for llm in llms:\n",
    "            folder_path = f\"{folder}/{llm}\"\n",
    "            if approach == \"greedy\":\n",
    "                evaluator = TaxonomyEvaluator(\n",
    "                    folder_path,\n",
    "                    dataset,\n",
    "                    ground_truth_path,\n",
    "                    num_generation,\n",
    "                    evaluate_greedy=True,\n",
    "                )\n",
    "                metrics = evaluator.evaluate_abstraction(\n",
    "                    num_generation, concretization_method=\"mv\", dataset=dataset\n",
    "                )\n",
    "            else:\n",
    "                evaluator = TaxonomyEvaluator(\n",
    "                    folder_path, dataset, ground_truth_path, num_generation\n",
    "                )\n",
    "                df = pd.read_csv(\n",
    "                    f\"{folder_path}/{dataset}/results_{approach}_{num_generation}.csv\"\n",
    "                )\n",
    "                metrics = evaluator.evaluate_taxonomies(df, dataset)\n",
    "            result[approach][llm] = metrics\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for num_generation in tqdm(num_generations):\n",
    "    result = get_result(ground_truth_folder, dataset, llms, num_generation, approaches, folder)\n",
    "\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(results))):\n",
    "    results[i][\"max\"] = {}\n",
    "    results[i][\"median\"] = {}\n",
    "    for llm in llms:\n",
    "        folder_path = f\"{folder}/{llm}\"\n",
    "        evaluator = TaxonomyEvaluator(\n",
    "            folder_path,\n",
    "            dataset,\n",
    "            ground_truth_path,\n",
    "            num_generations=len(list(num_generations)),\n",
    "        )\n",
    "\n",
    "        results[i][\"max\"][llm] = evaluator.evaluate_individual(i + 1, dataset, aggregator=max)\n",
    "        results[i][\"median\"][llm] = evaluator.evaluate_individual(i + 1, dataset, aggregator=np.median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "import matplotlib\n",
    "plt.style.use(['science', \"ieee\"])\n",
    "\n",
    "# models = [\"llama_8b\", \"llama_70b\", \"gpt_4o_mini\", \"gpt_4o\"]\n",
    "models = [\"meta-llama-3-1-70b-instruct-20241203161536\"]# [\"Meta-Llama-3.1-70B-Instruct\"]\n",
    "model_names = [\"Llama3.1 70b\"]\n",
    "metrics = [\"f1\", \"consistency\"]\n",
    "approaches = [\"mv\", \"abscon\", \"max\", \"median\", \"greedy\"]\n",
    "approach_names = [\"MV\", \"AbsCon\", \"Best\", \"Median\", \"Direct\"]\n",
    "lines = [\"-\", \"-\", \"--\", \"--\", \"-\"]\n",
    "markers = ['*', '.', '^', 'v', '']\n",
    "\n",
    "colors = [[33, 25, 24], [195, 56, 40], [71, 133, 90] , [71, 133, 90],  [231, 189, 57]]\n",
    "colors = [[c / 255 for c in color] for color in colors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,1.5))\n",
    "metric = \"f1\"\n",
    "x = num_generations\n",
    "f1_values = []\n",
    "for i, llm in enumerate(models):\n",
    "    for j, approach in enumerate(approaches):\n",
    "        values = [data[approach][llm][metric] for data in results]\n",
    "        if approach in [\"mv\", \"abscon\"]:\n",
    "            f1_values.extend(values)\n",
    "        plt.plot(x, values, color=colors[j], linestyle=lines[j], label=approach_names[j], marker=markers[j])\n",
    "plt.legend(shadow=True, ncol=2)\n",
    "plt.title(\"CCS\")\n",
    "plt.ylabel(\"F1\")\n",
    "plt.xlabel(\"Candidates\")\n",
    "plt.savefig(\"CCS.png\", dpi=300)\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,2.25))\n",
    "metric = \"consistency\"\n",
    "approaches = [\"mv\", \"abscon\", \"greedy\"]\n",
    "x = num_generations\n",
    "consistency_values = []\n",
    "for i, llm in enumerate(models):\n",
    "    for j, approach in enumerate(approaches):\n",
    "        values = [data[approach][llm][metric] for data in results]\n",
    "        if approach != \"greedy\":\n",
    "            consistency_values.extend(values)\n",
    "        plt.plot(x, values, color=colors[j], linestyle=lines[i])\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "spearmanr(f1_values, consistency_values, alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "spearmanr(f1_values, consistency_values, alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ3: Impact of temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_results(results):\n",
    "    transformed = []\n",
    "    for temperature in results.keys():\n",
    "        temperature_result = {\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        for approach in results[temperature].keys():\n",
    "            for llm in results[temperature][approach].keys():\n",
    "                for metric in results[temperature][approach][llm].keys():\n",
    "                    temperature_result[f\"{metric}_{llm}\"] = results[temperature][\n",
    "                        approach\n",
    "                    ][llm][metric] \n",
    "        transformed.append(temperature_result)\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"results\"\n",
    "ground_truth_folder = \"data\"\n",
    "approaches = [\"abscon\"]\n",
    "llms = [\"Meta-Llama-3.1-70B-Instruct\", \"gpt-4o-mini\"]\n",
    "\n",
    "num_generation = 10\n",
    "\n",
    "temperatures = [\"0.2\", \"0.5\", \"0.7\", \"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"wordnet\"\n",
    "\n",
    "temperature_results = {}\n",
    "\n",
    "for temperature in temperatures:\n",
    "    temperature_folder = f\"{folder}/temperature/{temperature}\"\n",
    "    temperature_results[temperature] = get_result(\n",
    "        ground_truth_folder, dataset, llms, num_generation, approaches, temperature_folder\n",
    "    )\n",
    "temperature_results = transform_results(temperature_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(temperature_results)\n",
    "df[df.select_dtypes(include=[\"number\"]).columns] *= 100\n",
    "df = df[\n",
    "    [\n",
    "        \"temperature\",\n",
    "        \"f1_Meta-Llama-3.1-70B-Instruct\",\n",
    "        \"consistency_Meta-Llama-3.1-70B-Instruct\",\n",
    "        \"f1_gpt-4o-mini\",\n",
    "        \"consistency_gpt-4o-mini\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(df.round(2).to_latex(index=False, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"ccs\"\n",
    "\n",
    "temperature_results = {}\n",
    "\n",
    "for temperature in temperatures:\n",
    "    temperature_folder = f\"{folder}/temperature/{temperature}\"\n",
    "    temperature_results[temperature] = get_result(\n",
    "        ground_truth_folder, dataset, llms, num_generation, approaches, temperature_folder\n",
    "    )\n",
    "temperature_results = transform_results(temperature_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(temperature_results)\n",
    "df[df.select_dtypes(include=[\"number\"]).columns] *= 100\n",
    "df = df[\n",
    "    [\n",
    "        \"temperature\",\n",
    "        \"f1_Meta-Llama-3.1-70B-Instruct\",\n",
    "        \"consistency_Meta-Llama-3.1-70B-Instruct\",\n",
    "        \"f1_gpt-4o-mini\",\n",
    "        \"consistency_gpt-4o-mini\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(df.round(2).to_latex(index=False, header=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot other figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1, 11)\n",
    "\n",
    "\n",
    "def plot_metric(metric, row, col, index, results, type, legend=True, title=True):\n",
    "    plt.subplot(row, col, index)\n",
    "    for i, model in enumerate(models):\n",
    "        plt.plot(\n",
    "            x,\n",
    "            [data[metric] for data in results[model]],\n",
    "            lines[i],\n",
    "            label=model_names[i],\n",
    "            linewidth=2,\n",
    "            # color=colors[i]\n",
    "        )\n",
    "        plt.xticks(x, [1,2,3,4,5,6,7,8,9,10]) \n",
    "        plt.annotate\n",
    "    if legend:\n",
    "        plt.legend()\n",
    "    if title:\n",
    "        plt.title(f\"{metric.title()} v.s. Number of Candidates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family' : 'normal',\n",
    "        'size'   : 12}\n",
    "\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4.5))\n",
    "plot_metric(\"consistency\", 2, 2, 1, result_changes, \"Constraints\", legend=True)\n",
    "plot_metric(\"f1\", 2, 2, 2, result_changes, \"Constraints\", legend=False)\n",
    "plot_metric(\"consistency\", 2, 2, 3, result_changes_mv, \"Majority Voting\", legend=False, title=False)\n",
    "plot_metric(\"f1\", 2, 2, 4, result_changes_mv, \"Majority Voting\", legend=False, title=False)\n",
    "\n",
    "plt.savefig(\"results.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1, 6)\n",
    "\n",
    "for metric in metrics:\n",
    "    for model in models:\n",
    "        plt.plot(x, [data[metric] for data in result_changes[model]], label=model)\n",
    "    plt.legend()\n",
    "    plt.title(f\"{metric} change w.r.t. samples (constraint)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(1, 6)\n",
    "\n",
    "for metric in metrics:\n",
    "    for model in models:\n",
    "        plt.plot(x, [data[metric] for data in result_changes_mv[model]])\n",
    "    plt.title(f\"{metric} change w.r.t. samples (voting)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of DeepSeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = \"gpt-4o-mini/\"\n",
    "dataset = \"wordnet\"\n",
    "folder = \"results\"\n",
    "folder_path = f\"{folder}/{llm}\"\n",
    "ground_truth_folder = \"data\"\n",
    "ground_truth_path = f\"{ground_truth_folder}/{dataset}.csv\"\n",
    "num_generation = 1\n",
    "\n",
    "evaluator = TaxonomyEvaluator(\n",
    "    folder_path,\n",
    "    dataset,\n",
    "    ground_truth_path,\n",
    "    num_generation,\n",
    "    evaluate_greedy=True,\n",
    ")\n",
    "metrics = evaluator.evaluate_abstraction(\n",
    "    num_generation, concretization_method=\"mv\", dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abscon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
