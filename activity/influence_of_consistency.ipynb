{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from loguru import logger\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the result for each candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_utils import ActivityEvaluator\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"results\"\n",
    "ground_truth_folder = \"data\"\n",
    "llms = [\"gpt-4o-mini\", \"gpt-4o\", \"Meta-Llama-3.1-8B-Instruct\", \"Meta-Llama-3.1-70B-Instruct\"]\n",
    "llm_labels = [\n",
    "    \"gpt-4o-mini\", \"gpt-4o\", \"Llama3.1-8b\", \"Llama3.1-70b\"\n",
    "]\n",
    "\n",
    "dataset = \"paged\"\n",
    "num_generations = range(1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ActivityEvaluator(\"\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_results = {}\n",
    "for llm in llms:\n",
    "    results = {\n",
    "        \"con\": [],\n",
    "        \"incon\": []\n",
    "    }\n",
    "    folder_path = f\"{folder}/{llm}\"\n",
    "    for num_generation in tqdm(num_generations):\n",
    "        df = pd.read_csv(f\"{folder_path}/{dataset}/results_{num_generation}.csv\")[\n",
    "            \"0\"\n",
    "        ].tolist()\n",
    "        evaluation_results = evaluator.evaluate_solutions(df, return_value=\"all\")\n",
    "        num_samples = len(evaluation_results[\"f1\"])\n",
    "        f1_con = np.mean(\n",
    "            [\n",
    "                evaluation_results[\"f1\"][i]\n",
    "                for i in range(num_samples)\n",
    "                if evaluation_results[\"consistency\"][i]\n",
    "            ]\n",
    "        )\n",
    "        f1_incon = np.mean(\n",
    "            [\n",
    "                evaluation_results[\"f1\"][i]\n",
    "                for i in range(num_samples)\n",
    "                if not evaluation_results[\"consistency\"][i]\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        results[\"con\"].append(f1_con)\n",
    "        results[\"incon\"].append(f1_incon)\n",
    "    llm_results[llm] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_generation = 10\n",
    "abscon_result = {}\n",
    "for llm in tqdm(llms):\n",
    "    folder_path = f\"{folder}/{llm}\"\n",
    "    df = pd.read_csv(f\"{folder_path}/{dataset}/results_abscon_{num_generation}.csv\")[\n",
    "        \"0\"\n",
    "    ].tolist()\n",
    "    evaluation_results = evaluator.evaluate_solutions(df, return_value=\"avg\")\n",
    "    # print(len(evaluation_results[\"f1\"]))\n",
    "    results = {\n",
    "        \"abscon\": [evaluation_results[\"f1\"]]\n",
    "    }\n",
    "\n",
    "    abscon_result[llm] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the difference between consistent and non-consistent candidates\n",
    "llm_results_categorization = {\n",
    "    \"abscon\": [],\n",
    "    \"consistent\": [],\n",
    "    \"inconsistent\": []\n",
    "}\n",
    "\n",
    "for llm in llms:\n",
    "    llm_results_categorization[\"abscon\"].append(abscon_result[llm][\"abscon\"])\n",
    "    llm_results_categorization[\"consistent\"].append(llm_results[llm][\"con\"])\n",
    "    llm_results_categorization[\"inconsistent\"].append(llm_results[llm][\"incon\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abscon_result[llm][\"abscon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abscon_results = llm_results_categorization[\"abscon\"] \n",
    "abscon_plot = plt.boxplot(abscon_results,\n",
    "                               positions=np.array(\n",
    "    np.arange(len(abscon_results)))*3.0-0.8, \n",
    "                               widths=0.6)\n",
    "\n",
    "\n",
    "consistent_results = llm_results_categorization[\"consistent\"] \n",
    "consistent_plot = plt.boxplot(consistent_results,\n",
    "                               positions=np.array(\n",
    "    np.arange(len(consistent_results)))*3.0-0, \n",
    "                               widths=0.6)\n",
    "\n",
    "inconsistent_results = llm_results_categorization[\"inconsistent\"] \n",
    "inconsistent_plot = plt.boxplot(inconsistent_results,\n",
    "                               positions=np.array(\n",
    "    np.arange(len(inconsistent_results)))*3.0+0.8, \n",
    "                               widths=0.6)\n",
    "\n",
    "def define_box_properties(plot_name, color_code, label):\n",
    "    for k, v in plot_name.items():\n",
    "        plt.setp(plot_name.get(k), color=color_code)\n",
    "         \n",
    "    # use plot function to draw a small line to name the legend.\n",
    "    plt.plot([], c=color_code, label=label)\n",
    "    plt.legend()\n",
    "\n",
    "# setting colors for each groups\n",
    "define_box_properties(abscon_plot, '#F5B841', 'AbsCon')\n",
    "define_box_properties(consistent_plot, '#067BC2', 'Consistent')\n",
    "define_box_properties(inconsistent_plot, '#E84855', 'Inconsistent')\n",
    "\n",
    "plt.xticks(np.arange(0, len(llm_labels) * 3, 3), llm_labels)\n",
    "plt.xlim(-2, len(llm_labels)*2.7)\n",
    "\n",
    "plt.ylabel(\"f1 score\")\n",
    "plt.title(\"Paged\")\n",
    "# plt.savefig(f\"Paged.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cliffs_delta import cliffs_delta, lookup_size\n",
    "from scipy.stats import ranksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_test_results = []\n",
    "for consistent_result, inconsistent_result in zip(consistent_results, inconsistent_results):\n",
    "    statistical_test_results.append(ranksums(consistent_result, inconsistent_result, alternative=\"greater\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dull = {'small': 0.147, 'medium': 0.33, 'large': 0.474} # effect sizes from (Hess and Kromrey, 2004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliffs_values = []\n",
    "for consistent_result, inconsistent_result in zip(consistent_results, inconsistent_results):\n",
    "    cliffs_values.append(cliffs_delta(consistent_result, inconsistent_result)[0])\n",
    "mean_cliff = np.mean(cliffs_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliffs_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cliff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_size(mean_cliff, dull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abscon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
