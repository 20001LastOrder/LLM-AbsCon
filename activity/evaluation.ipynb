{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from loguru import logger\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"https_proxy\"] = os.getenv(\"https_proxyOPT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evaluation_utils import ActivityEvaluator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(ground_truth_folder, dataset, llms, num_generation, approaches, folder):\n",
    "    results = []\n",
    "\n",
    "    for approach in approaches:\n",
    "        ground_truth_path = f\"{ground_truth_folder}/{dataset}.csv\"\n",
    "\n",
    "        result = {}\n",
    "        result[\"approach\"] = approach\n",
    "        for llm in llms:\n",
    "            folder_path = f\"{folder}/{llm}\"\n",
    "            if approach == \"greedy\":\n",
    "                evaluator = ActivityEvaluator(\n",
    "                    folder_path,\n",
    "                    dataset,\n",
    "                )\n",
    "                metrics = evaluator.evaluate_greedy_result()\n",
    "            else:\n",
    "                evaluator = ActivityEvaluator(\n",
    "                    folder_path, dataset\n",
    "                )\n",
    "                df = pd.read_csv(f\"{folder_path}/{dataset}/results_{approach}_{num_generation}.csv\")[\"0\"].tolist()\n",
    "                metrics = evaluator.evaluate_solutions(df)\n",
    "            \n",
    "\n",
    "            for metric_name in [\"precision\", \"recall\", \"f1\", \"consistency\"]:\n",
    "                result[f\"{metric_name}_{llm}\"] = metrics[metric_name]\n",
    "        results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"results\"\n",
    "ground_truth_folder = \"data\"\n",
    "approaches = [\"greedy\", \"mv\", \"abscon\"]\n",
    "llms = [\"gpt-4o-mini\", \"gpt-4o\", \"Meta-Llama-3.1-8B-Instruct\", \"Meta-Llama-3.1-70B-Instruct\"]\n",
    "\n",
    "dataset = \"paged\"\n",
    "num_generation = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_result(ground_truth_folder, dataset, llms, num_generation, approaches, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df[results_df.select_dtypes(include=['number']).columns] *= 100\n",
    "results_df = results_df[[\"f1_gpt-4o-mini\", \"f1_gpt-4o\",\"f1_Meta-Llama-3.1-8B-Instruct\", \"f1_Meta-Llama-3.1-70B-Instruct\"]]\n",
    "print(results_df.round(2).to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.loc[2] - results_df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df[results_df.select_dtypes(include=['number']).columns] *= 100\n",
    "# results_df = results_df[[\"approach\", \"precision\", \"recall\", \"f1\", \"consistency\"]]\n",
    "# results_df.columns = [\"approach\", \"P\", \"R\", \"F1\", \"Con\"]\n",
    "print(results_df.round(2).to_latex(index=False, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df\n",
    "results_df_index = results_df.set_index(\"approach\", inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = results_df_index.loc[\"abscon\", :] - results_df_index.loc[\"greedy\", :]\n",
    "recall_diff = [value for key, value in diff.items() if \"recall\" in key]\n",
    "print(f\"Min improvement: {min(recall_diff)}, max improvement: {max(recall_diff)}, average improvement: {np.mean(recall_diff)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_diff = [value for key, value in diff.items() if \"f1\" in key]\n",
    "print(f\"Min improvement: {min(f1_diff)}, max improvement: {max(f1_diff)}, average improvement: {np.mean(f1_diff)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from evaluation_utils import ActivityEvaluator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"results\"\n",
    "ground_truth_folder = \"data\"\n",
    "approaches = [\"mv\", \"greedy\", \"abscon\"]\n",
    "llms = [\"Meta-Llama-3.1-70B-Instruct\"]# [\"Meta-Llama-3.1-70B-Instruct\"]\n",
    "\n",
    "dataset = \"paged\"\n",
    "num_generations = range(1, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for num_generation in tqdm(num_generations):\n",
    "    result = {}\n",
    "    for approach in approaches:\n",
    "        ground_truth_path = f\"{ground_truth_folder}/{dataset}.csv\"\n",
    "\n",
    "        result[approach] = {}\n",
    "        for llm in llms:\n",
    "            folder_path = f\"{folder}/{llm}\"\n",
    "            if approach == \"greedy\":\n",
    "                evaluator = ActivityEvaluator(\n",
    "                    folder_path,\n",
    "                    dataset,\n",
    "                    ground_truth_path,\n",
    "                )\n",
    "                metrics = evaluator.evaluate_greedy_result()\n",
    "            else:\n",
    "                evaluator = ActivityEvaluator(\n",
    "                    folder_path, dataset, ground_truth_path\n",
    "                )\n",
    "                df = pd.read_csv(f\"{folder_path}/{dataset}/results_{approach}_{num_generation}.csv\")[\"0\"].tolist()\n",
    "                metrics = evaluator.evaluate_solutions(df)\n",
    "            result[approach][llm] = metrics\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for llm in llms:\n",
    "    for i in range(len(results)):\n",
    "        results[i][\"max\"] = {}\n",
    "        results[i][\"median\"] = {}\n",
    "    evaluator = ActivityEvaluator(\n",
    "        folder_path, dataset, ground_truth_path\n",
    "    )\n",
    "\n",
    "    for i in tqdm(range(len(results))):\n",
    "        results[i][\"max\"][llm] = evaluator.evaluate_individual(\n",
    "            i + 1, dataset=dataset, aggregator=max\n",
    "        )\n",
    "        results[i][\"median\"][llm] = evaluator.evaluate_individual(\n",
    "            i + 1, dataset=dataset, aggregator=np.median\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use(['science', \"ieee\"])\n",
    "\n",
    "# models = [\"llama_8b\", \"llama_70b\", \"gpt_4o_mini\", \"gpt_4o\"]\n",
    "models = [\"Meta-Llama-3.1-70B-Instruct\"]# [\"Meta-Llama-3.1-70B-Instruct\"]\n",
    "model_names = [\"Llama3.1 70b\"]\n",
    "metrics = [\"f1\", \"consistency\"]\n",
    "approaches = [\"mv\", \"abscon\", \"max\", \"median\", \"greedy\"]\n",
    "approach_names = [\"MV\", \"AbsCon\", \"Best\", \"Median\", \"Direct\"]\n",
    "lines = [\"-\", \"-\", \"--\", \"--\", \"-\"]\n",
    "markers = ['*', '.', '^', 'v', '']\n",
    "\n",
    "colors = [[33, 25, 24], [195, 56, 40], [71, 133, 90] , [71, 133, 90],  [231, 189, 57]]\n",
    "colors = [[c / 255 for c in color] for color in colors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,1.25))\n",
    "metric = \"f1\"\n",
    "x = num_generations\n",
    "f1_values = []\n",
    "for i, llm in enumerate(models):\n",
    "    for j, approach in enumerate(approaches):\n",
    "        values = [data[approach][llm][metric] for data in results]\n",
    "        if approach in [\"mv\", \"abscon\"]:\n",
    "            f1_values.extend(values)\n",
    "        plt.plot(x, values, color=colors[j], linestyle=lines[j], label=approach_names[j], marker=markers[j])\n",
    "# plt.legend(shadow=True, ncol=2)\n",
    "plt.title(\"Paged\")\n",
    "plt.ylabel(\"F1\")\n",
    "plt.xlabel(\"Candidates\")\n",
    "plt.savefig(\"Paged.png\", dpi=300)\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,2.25))\n",
    "metric = \"consistency\"\n",
    "x = range(1, 11)\n",
    "consistency_values = []\n",
    "for i, llm in enumerate(models):\n",
    "    for j, approach in enumerate(approaches):\n",
    "        values = [data[approach][llm][metric] for data in results]\n",
    "        if approach != \"greedy\":\n",
    "            consistency_values.extend(values)\n",
    "        plt.plot(x, values, color=colors[j], linestyle=lines[i])\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "spearmanr(f1_values, consistency_values, alternative=\"greater\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ3: Impact of Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"results\"\n",
    "ground_truth_folder = \"data\"\n",
    "approaches = [\"abscon\"]\n",
    "llms = [\"Meta-Llama-3.1-70B-Instruct\", \"gpt-4o-mini\"]\n",
    "\n",
    "dataset = \"paged\"\n",
    "num_generation = 10\n",
    "\n",
    "temperatures = [\"0.2\", \"0.5\", \"0.7\", \"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_results = {}\n",
    "\n",
    "for temperature in temperatures:\n",
    "    temperature_folder = f\"{folder}/temperature/{temperature}\"\n",
    "    temperature_results[temperature] = get_result(\n",
    "        ground_truth_folder, dataset, llms, num_generation, approaches, temperature_folder\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_results(results):\n",
    "    transformed = []\n",
    "    for temperature in results.keys():\n",
    "        temperature_result = {\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        for key in results[temperature][0].keys():\n",
    "            temperature_result[key] = results[temperature][0][key]\n",
    "        transformed.append(temperature_result)\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_results = transform_results(temperature_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(temperature_results)\n",
    "df[df.select_dtypes(include=[\"number\"]).columns] *= 100\n",
    "df = df[\n",
    "    [\n",
    "        \"temperature\",\n",
    "        \"f1_Meta-Llama-3.1-70B-Instruct\",\n",
    "        \"consistency_Meta-Llama-3.1-70B-Instruct\",\n",
    "        \"f1_gpt-4o-mini\",\n",
    "        \"consistency_gpt-4o-mini\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(df.round(2).to_latex(index=False, header=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abscon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
